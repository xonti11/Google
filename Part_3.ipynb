{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn import metrics\n",
    "from catboost import Pool,CatBoostRegressor,CatBoostClassifier\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "patch=\"D:\\Python\\\\Google\"\n",
    "def run_lgb(train_x, train_y, val_x, val_y,val_id):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"learning_rate\" : 0.02,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 50,\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(50/3), verbose_eval=100)\n",
    "    \n",
    "    pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"transactionRevenue\":val_y,\"PredictedRevenue\":pred_val})\n",
    "#    return model\n",
    "    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "def run_cat(train_x, train_y, val_x, val_y,val_id):\n",
    "    pool=Pool(train_x,train_y)\n",
    "    val_pool=Pool(val_x,val_y)\n",
    "    model = CatBoostRegressor(iterations=50, learning_rate=0.01,loss_function='RMSE', eval_metric='RMSE')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    \n",
    "    pred_val = model.predict(val_x)\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"transactionRevenue\":val_y,\"PredictedRevenue\":pred_val})\n",
    "    return model\n",
    "#    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "\n",
    "def run_cat_bin(train_x, train_y, val_x, val_y,val_id):\n",
    "    pool=Pool(train_x,train_y)\n",
    "    val_pool=Pool(val_x,val_y)\n",
    "    model = CatBoostClassifier(iterations=80, learning_rate=0.05,loss_function='CrossEntropy', eval_metric='AUC')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    pred_train=model.predict(train_x, prediction_type=\"Probability\")[:,1]\n",
    "    pred_val = model.predict(val_x, prediction_type=\"Probability\")[:,1]\n",
    "#    return val_pred_df\n",
    "    return  pred_train,pred_val\n",
    "\n",
    "def run_lgb_bin(train_x, train_y, val_x, val_y,val_id):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        \"learning_rate\" : 0.02,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 50,\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(50/3), verbose_eval=100)\n",
    "    \n",
    "    pred_train = model.predict(train_x, num_iteration=model.best_iteration)\n",
    "    pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    \n",
    "#    return model\n",
    "    return  pred_train,pred_val\n",
    "\n",
    "    \n",
    "def multimodel(train_x, train_y, val_x, val_y):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"learning_rate\" : 0.01,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 70,\n",
    "    }\n",
    "    i=random.randint(0,99999)\n",
    "    train1=pd.concat([train_x.reset_index(drop=True),pd.DataFrame(train_y,columns=['bg'])],axis=1)\n",
    "    train_x_bad=train1[train1['bg']>0].drop(['bg'],axis=1)\n",
    "    train_y_bad=train1[train1['bg']>0]['bg'].values\n",
    "    train_x_good=train1[train1['bg']==0].drop(['bg'],axis=1).sample(len(train_x_bad)*6,random_state=i)\n",
    "    train_y_good=train1[train1['bg']==0]['bg'].sample(len(train_x_bad)*6,random_state=i).values\n",
    "    \n",
    "    val1=pd.concat([val_x.reset_index(drop=True),pd.DataFrame(val_y,columns=['bg'])],axis=1)\n",
    "    val_x_bad=val1[val1['bg']>0].drop(['bg'],axis=1)\n",
    "    val_y_bad=val1[val1['bg']>0]['bg'].values\n",
    "    val_x_good=val1[val1['bg']==0].drop(['bg'],axis=1).sample(len(val_x_bad)*6,random_state=i)\n",
    "    val_y_good=val1[val1['bg']==0]['bg'].sample(len(val_x_bad)*6,random_state=i).values\n",
    "    \n",
    "    train_x,train_y=pd.concat([train_x_bad,train_x_good]),np.concatenate([train_y_bad,train_y_good])\n",
    "    val_x,val_y=pd.concat([val_x_bad,val_x_good]),np.concatenate([val_y_bad,val_y_good])\n",
    "#     pool=Pool(train_x,train_y)\n",
    "#     val_pool=Pool(val_x,val_y)\n",
    "#     model = CatBoostRegressor(iterations=50, learning_rate=0.01,loss_function='RMSE', eval_metric='RMSE')\n",
    "#     model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(50/3), verbose_eval=100)\n",
    "    return model\n",
    "\n",
    "def run_multimodel(train_x, train_y, val_x, val_y,val_id):\n",
    "    result_val=[]\n",
    "    for i in range(0,10):\n",
    "        model=multimodel(train_x, train_y, val_x, val_y)\n",
    "        result_val.append(model.predict(val_x, num_iteration=model.best_iteration))\n",
    "    pred_val = sum(result_val)/float(len(result_val))\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_id.values})\n",
    "    val_pred_df[\"transactionRevenue\"] = val_y\n",
    "    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n",
    "    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "#    return val_pred_df\n",
    "    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "\n",
    "def grouping_month(df):\n",
    "    df[\"date\"]=pd.to_datetime(df[\"visitStartTime\"], format='%Y%m%d')\n",
    "    df['Month']=df[\"date\"].dt.to_period(\"M\")\n",
    "    df['totals.hits']=df['totals.hits'].astype(int)\n",
    "    df['totals.newVisits'].fillna(0, inplace=True)\n",
    "    df['totals.newVisits']=df['totals.newVisits'].astype(int)\n",
    "    df['totals.pageviews'].fillna(0, inplace=True)\n",
    "    df['totals.pageviews']=df['totals.pageviews'].astype(int)\n",
    "    df['totals.bounces'].fillna(0, inplace=True)\n",
    "    df['totals.bounces']=df['totals.bounces'].astype(int)\n",
    "    df['totals.totalTransactionRevenue'].fillna(0, inplace=True)\n",
    "    df['totals.totalTransactionRevenue']=df['totals.totalTransactionRevenue'].astype(float)\n",
    "    aggs={\n",
    "        'totals.totalTransactionRevenue': ['sum','count'],\n",
    "        'totals.hits': ['sum','count', 'median'],\n",
    "        'totals.pageviews': ['sum','mean', 'median'],\n",
    "        'totals.bounces': ['sum', 'mean', 'median'],\n",
    "        'totals.newVisits': ['sum', 'mean', 'median']\n",
    "    }\n",
    "    temp=df.groupby(['fullVisitorId','Month'],as_index=False).agg({**aggs})\n",
    "    col=[\"_month_\".join(x) for x in temp.columns.ravel()[2:]]\n",
    "    col.insert(0,'Month')\n",
    "    col.insert(0,'fullVisitorId')\n",
    "    temp.columns=col\n",
    "    #temp.columns=['fullVisitorId','Month','transaction_month_count']\n",
    "    df=pd.merge(df,temp,on=['fullVisitorId','Month'],how='left')\n",
    "    return df\n",
    "def grouping_all(df):\n",
    "    aggs={\n",
    "        'totals.totalTransactionRevenue': ['sum'],\n",
    "        'totals.hits': ['sum','count', 'median'],\n",
    "        'totals.pageviews': ['sum','mean', 'median'],\n",
    "        'totals.bounces': ['sum', 'mean', 'median'],\n",
    "        'totals.newVisits': ['sum', 'mean', 'median']\n",
    "    }\n",
    "    temp=df.groupby(['fullVisitorId'],as_index=False).agg({**aggs})\n",
    "    col=[\"_all_\".join(x) for x in temp.columns.ravel()[1:]]\n",
    "    col.insert(0,'fullVisitorId')\n",
    "    temp.columns=col\n",
    "    #temp.columns=['fullVisitorId','Month','transaction_month_count']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "def grouping_date(df,y,m,d):\n",
    "    temp=df.groupby('fullVisitorId',as_index=False)['visitStartTime'].max()\n",
    "    temp.columns=['fullVisitorId','last_visit']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    temp=df[df['totals.totalTransactionRevenue']>0].groupby('fullVisitorId',as_index=False)['visitStartTime'].max()\n",
    "    temp.columns=['fullVisitorId','last_visit_money']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    \n",
    "    temp=df.groupby('fullVisitorId',as_index=False)['visitStartTime'].min()\n",
    "    temp.columns=['fullVisitorId','duration_visit']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    temp=df[df['totals.totalTransactionRevenue']>0].groupby('fullVisitorId',as_index=False)['visitStartTime'].min()\n",
    "    temp.columns=['fullVisitorId','duration_visit_money']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    \n",
    "    dat_cols=['visitStartTime','last_visit','last_visit_money','duration_visit','duration_visit_money']\n",
    "    for col in dat_cols:\n",
    "        df[col] = pd.to_datetime(df[col],unit='s')\n",
    "    df['duration_visit']=df['last_visit']-df['duration_visit']\n",
    "    df['duration_visit']=df['duration_visit'].dt.days\n",
    "    df['duration_visit_money']=df['last_visit_money']-df['duration_visit_money']\n",
    "    df['duration_visit_money']=df['duration_visit_money'].dt.days\n",
    "    df['last_visit']=datetime.datetime(y,m,d)-df['last_visit']\n",
    "    df['last_visit']=df['last_visit'].dt.days\n",
    "    df['last_visit_money']=datetime.datetime(y,m,d)-df['last_visit_money']\n",
    "    df['last_visit_money']=df['last_visit_money'].dt.days\n",
    "    return df\n",
    "\n",
    "def grouping_id(df):\n",
    "    aggs={\n",
    "        'totals.totalTransactionRevenue': ['sum'],\n",
    "        'totals.totalTransactionCount': ['sum','mean'],\n",
    "        'totals.hits': ['sum','count', 'median'],\n",
    "        'totals.pageviews': ['sum','mean', 'median'],\n",
    "        'totals.bounces': ['sum'],\n",
    "        'totals.newVisits': ['sum', 'mean'],\n",
    "        'customDimensions_value': ['max','min'],\n",
    "        'geoNetwork.subContinent': ['max','min'],\n",
    "        'geoNetwork.region': ['max','min'],\n",
    "        'geoNetwork.networkDomain': ['max','min'],\n",
    "        'geoNetwork.continent': ['max','min'],\n",
    "        'device.operatingSystem': ['max','min'],\n",
    "        'device.deviceCategory': ['max','min'],\n",
    "        'channelGrouping': ['max','min'],\n",
    "        'visitNumber': ['max'],\n",
    "        'last_visit_money': ['max'],\n",
    "        'last_visit': ['max']\n",
    "#         'duration_visit': ['max']\n",
    "#         'duration_visit_money': ['max']\n",
    "    }\n",
    "    temp=df.groupby(['fullVisitorId'],as_index=False).agg({**aggs})\n",
    "    col=[\"_\".join(x) for x in temp.columns.ravel()[1:]]\n",
    "    col.insert(0,'fullVisitorId')\n",
    "    temp.columns=col\n",
    "    temp.fillna(0, inplace=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_pickle(os.path.join(patch,'train_v2_clean.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channelGrouping\n",
      "device.browser\n",
      "device.deviceCategory\n",
      "device.operatingSystem\n",
      "geoNetwork.city\n",
      "geoNetwork.continent\n",
      "geoNetwork.country\n",
      "geoNetwork.metro\n",
      "geoNetwork.networkDomain\n",
      "geoNetwork.region\n",
      "geoNetwork.subContinent\n",
      "trafficSource.adContent\n",
      "trafficSource.adwordsClickInfo.adNetworkType\n",
      "trafficSource.adwordsClickInfo.gclId\n",
      "trafficSource.adwordsClickInfo.page\n",
      "trafficSource.adwordsClickInfo.slot\n",
      "trafficSource.campaign\n",
      "trafficSource.keyword\n",
      "trafficSource.medium\n",
      "trafficSource.referralPath\n",
      "trafficSource.source\n",
      "trafficSource.adwordsClickInfo.isVideoAd\n",
      "trafficSource.isTrueDirect\n",
      "customDimensions_value\n",
      "weekday\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "#train=grouping_month(train)\n",
    "train[\"totals.totalTransactionRevenue\"].fillna(0, inplace=True)\n",
    "train[\"visitStartTime\"] = pd.to_datetime(train[\"visitStartTime\"],unit='s')\n",
    "\n",
    "train['day']=train['visitStartTime'].dt.day.astype('uint8')\n",
    "train['weekday']=train['visitStartTime'].apply(lambda x:x.date().weekday())\n",
    "train['minute']=train['visitStartTime'].dt.minute.astype('uint8')\n",
    "train['hour']=train['visitStartTime'].dt.hour.astype('uint8')\n",
    "\n",
    "# label encode the categorical variables and convert the numerical variables to float\n",
    "#ar=df['geoNetwork.networkDomain'].value_counts()[df['geoNetwork.networkDomain'].value_counts()<=4].index\n",
    "#train['geoNetwork.networkDomain']=['some unfamous domain' if x in ar else x for x in df['geoNetwork.networkDomain'].values]\n",
    "#del ar\n",
    "cat_cols = [\"channelGrouping\", \"device.browser\", \n",
    "            \"device.deviceCategory\", \"device.operatingSystem\", \n",
    "            \"geoNetwork.city\", \"geoNetwork.continent\", \n",
    "            \"geoNetwork.country\", \"geoNetwork.metro\",\n",
    "            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n",
    "            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n",
    "            \"trafficSource.adwordsClickInfo.adNetworkType\", \n",
    "            \"trafficSource.adwordsClickInfo.gclId\", \n",
    "            \"trafficSource.adwordsClickInfo.page\", \n",
    "            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n",
    "            \"trafficSource.keyword\", \"trafficSource.medium\", \n",
    "            \"trafficSource.referralPath\", \"trafficSource.source\",\n",
    "            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect',\n",
    "           'customDimensions_value','weekday']\n",
    "for col in cat_cols:\n",
    "    print(col)\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[col].values.astype('str')))\n",
    "    train[col] = lbl.transform(list(train[col].values.astype('str')))\n",
    "#train=pd.get_dummies(train,columns=cat_cols,prefix=cat_cols,prefix_sep='_')\n",
    "\n",
    "num_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", 'totals.bounces',  'totals.newVisits',\n",
    "            'totals.transactions','totals.sessionQualityDim','totals.timeOnSite','day','minute','hour']\n",
    "\n",
    "# num_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", 'totals.bounces',  'totals.newVisits',\n",
    "#            'totals.transactions','totals.sessionQualityDim','totals.timeOnSite','day','minute','hour',\n",
    "#             'totals.totalTransactionRevenue_month_sum',\n",
    "#            'totals.totalTransactionRevenue_month_count', 'totals.hits_month_sum',\n",
    "#            'totals.hits_month_count', 'totals.hits_month_median',\n",
    "#            'totals.pageviews_month_sum', 'totals.pageviews_month_mean',\n",
    "#            'totals.pageviews_month_median', 'totals.bounces_month_sum',\n",
    "#            'totals.bounces_month_mean', 'totals.bounces_month_median',\n",
    "#            'totals.newVisits_month_sum', 'totals.newVisits_month_mean',\n",
    "#            'totals.newVisits_month_median']    \n",
    "for col in num_cols:\n",
    "    train[col] = train[col].astype(float)\n",
    "    \n",
    "train['totals.totalTransactionRevenue'] = train['totals.totalTransactionRevenue'].astype(float)\n",
    "train['totals.totalTransactionCount'] = np.where(train['totals.totalTransactionRevenue'].values>0,1,0)\n",
    "train['device.isMobile']=train['device.isMobile'].astype(int)\n",
    "\n",
    "# Split the train dataset into development and valid based on time \n",
    "visitid=train['fullVisitorId']\n",
    "train=train.drop(['date','visitId','totals.transactionRevenue'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\lightgbm\\engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid_0's rmse: 0.449797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07311585429052049"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# здесь все тестим\n",
    "import time\n",
    "import datetime\n",
    "train_x_index=train[(train['visitStartTime']>=datetime.date(2016,12,1))&(train['visitStartTime']<datetime.date(2017,5,1))].index\n",
    "train_y_index=train[(train['visitStartTime']>=datetime.date(2017,6,1))&(train['visitStartTime']<datetime.date(2017,8,1))].index\n",
    "val_x_index=train[(train['visitStartTime']>=datetime.date(2017,8,1))&(train['visitStartTime']<datetime.date(2017,11,1))].index\n",
    "val_y_index=train[(train['visitStartTime']>=datetime.date(2017,12,1))&(train['visitStartTime']<datetime.date(2018,2,1))].index\n",
    "\n",
    "df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "#df_train_x=grouping_id(train.loc[train_x_index])\n",
    "df_train_x=grouping_id(grouping_date(train.loc[train_x_index],2016,11,1))\n",
    "train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "#train_x=grouping_all(train.loc[train_x_index])\n",
    "#train_x=grouping_date(df_train_x,2016,11,1)\n",
    "#train_x=train_x.drop(['fullVisitorId'],axis=1)\n",
    "\n",
    "df_val_y=train.drop(['visitStartTime'],axis=1).loc[val_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_val_y=df_val_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "#df_val_x=grouping_id(train.loc[val_x_index])\n",
    "df_val_x=grouping_id(grouping_date(train.loc[val_x_index],2017,11,1))\n",
    "val_y=pd.merge(df_val_x,df_val_y,how='left',on='fullVisitorId')\n",
    "val_y=np.log1p(val_y['Revenue'].fillna(0).values)\n",
    "val_id=visitid[val_x_index]\n",
    "val_x=df_val_x.drop(['fullVisitorId'],axis=1)\n",
    "#val_x=grouping_all(train.loc[val_x_index])\n",
    "#val_x=grouping_date(df_val_x.drop(['fullVisitorId'],axis=1),2017,11,1)\n",
    "#val_x=val_x.drop(['fullVisitorId'],axis=1)\n",
    "\n",
    "run_lgb(train_x,train_y,val_x,val_y,val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.800612</td>\n",
       "      <td>totals.totalTransactionRevenue_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.667628</td>\n",
       "      <td>totals.totalTransactionCount_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.549334</td>\n",
       "      <td>totals.totalTransactionCount_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.341782</td>\n",
       "      <td>totals.hits_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.323554</td>\n",
       "      <td>totals.hits_count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.324233</td>\n",
       "      <td>totals.hits_median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.418842</td>\n",
       "      <td>totals.pageviews_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.358193</td>\n",
       "      <td>totals.pageviews_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.728381</td>\n",
       "      <td>totals.pageviews_median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.132310</td>\n",
       "      <td>totals.bounces_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.573513</td>\n",
       "      <td>totals.newVisits_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>totals.newVisits_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>customDimensions_value_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>customDimensions_value_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.subContinent_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.subContinent_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.258282</td>\n",
       "      <td>geoNetwork.region_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.257097</td>\n",
       "      <td>geoNetwork.region_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.790745</td>\n",
       "      <td>geoNetwork.networkDomain_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.206666</td>\n",
       "      <td>geoNetwork.networkDomain_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.continent_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.continent_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.015354</td>\n",
       "      <td>device.operatingSystem_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.253412</td>\n",
       "      <td>device.operatingSystem_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>device.deviceCategory_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>device.deviceCategory_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.156450</td>\n",
       "      <td>channelGrouping_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.305077</td>\n",
       "      <td>channelGrouping_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.932903</td>\n",
       "      <td>visitNumber_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.219619</td>\n",
       "      <td>last_visit_money_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11.231744</td>\n",
       "      <td>last_visit_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.737698</td>\n",
       "      <td>duration_visit_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6.416572</td>\n",
       "      <td>duration_visit_money_max</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                   1\n",
       "0   30.800612  totals.totalTransactionRevenue_sum\n",
       "1    0.667628    totals.totalTransactionCount_sum\n",
       "2    1.549334   totals.totalTransactionCount_mean\n",
       "3    4.341782                     totals.hits_sum\n",
       "4    0.323554                   totals.hits_count\n",
       "5    2.324233                  totals.hits_median\n",
       "6    2.418842                totals.pageviews_sum\n",
       "7    5.358193               totals.pageviews_mean\n",
       "8    2.728381             totals.pageviews_median\n",
       "9    0.132310                  totals.bounces_sum\n",
       "10   0.573513                totals.newVisits_sum\n",
       "11   0.000000               totals.newVisits_mean\n",
       "12   0.000000          customDimensions_value_max\n",
       "13   0.000000          customDimensions_value_min\n",
       "14   0.000000         geoNetwork.subContinent_max\n",
       "15   0.000000         geoNetwork.subContinent_min\n",
       "16   0.258282               geoNetwork.region_max\n",
       "17   1.257097               geoNetwork.region_min\n",
       "18   0.790745        geoNetwork.networkDomain_max\n",
       "19   0.206666        geoNetwork.networkDomain_min\n",
       "20   0.000000            geoNetwork.continent_max\n",
       "21   0.000000            geoNetwork.continent_min\n",
       "22   7.015354          device.operatingSystem_max\n",
       "23   2.253412          device.operatingSystem_min\n",
       "24   0.000000           device.deviceCategory_max\n",
       "25   0.000000           device.deviceCategory_min\n",
       "26   2.156450                 channelGrouping_max\n",
       "27   0.305077                 channelGrouping_min\n",
       "28   0.932903                     visitNumber_max\n",
       "29  12.219619                last_visit_money_max\n",
       "30  11.231744                      last_visit_max\n",
       "31   3.737698                  duration_visit_max\n",
       "32   6.416572            duration_visit_money_max"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(model.feature_importances_,train_x.columns.values)))\n",
    "#0.07631281498833459\n",
    "#0.07530680899781979\n",
    "#train.groupby(['fullVisitorId'],as_index=False)['totals.totalTransactionCount'].var().describe()\n",
    "#train_x['totals.pageviews_month_sum_mean']\n",
    "#result\n",
    "#train\n",
    "#0.07670706481404065\n",
    "#train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\lightgbm\\engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's rmse: 0.450402\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.451067\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's rmse: 0.450705\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's rmse: 0.45067\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.450484\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's rmse: 0.450645\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450724\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's rmse: 0.450671\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450684\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450724\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450724\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450724\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450718\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's rmse: 0.450726\n"
     ]
    }
   ],
   "source": [
    "train_x_index=train[(train['visitStartTime']>=datetime.date(2016,8,1))&(train['visitStartTime']<datetime.date(2016,11,1))].index\n",
    "train_y_index=train[(train['visitStartTime']>=datetime.date(2016,12,1))&(train['visitStartTime']<datetime.date(2017,2,1))].index\n",
    "val_x_index=train[(train['visitStartTime']>=datetime.date(2017,8,1))&(train['visitStartTime']<datetime.date(2017,11,1))].index\n",
    "val_y_index=train[(train['visitStartTime']>=datetime.date(2017,12,1))&(train['visitStartTime']<datetime.date(2018,2,1))].index\n",
    "\n",
    "df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "#df_train_x=grouping_id(train.loc[train_x_index])\n",
    "df_train_x=grouping_id(grouping_date(grouping_month(train.loc[train_x_index]),2016,11,1))\n",
    "train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "#train_x=grouping_all(train.loc[train_x_index])\n",
    "#train_x=grouping_date(df_train_x,2016,11,1)\n",
    "#train_x=train_x.drop(['fullVisitorId'],axis=1)\n",
    "\n",
    "df_val_y=train.drop(['visitStartTime'],axis=1).loc[val_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_val_y=df_val_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "#df_val_x=grouping_id(train.loc[val_x_index])\n",
    "df_val_x=grouping_id(grouping_date(grouping_month(train.loc[val_x_index]),2017,11,1))\n",
    "val_y=pd.merge(df_val_x,df_val_y,how='left',on='fullVisitorId')\n",
    "val_y=np.log1p(val_y['Revenue'].fillna(0).values)\n",
    "val_id=visitid[val_x_index]\n",
    "val_x=df_val_x.drop(['fullVisitorId'],axis=1)\n",
    "#val_x=grouping_all(train.loc[val_x_index])\n",
    "#val_x=grouping_date(df_val_x.drop(['fullVisitorId'],axis=1),2017,11,1)\n",
    "#val_x=val_x.drop(['fullVisitorId'],axis=1)\n",
    "h=['totals.totalTransactionRevenue_month_sum_mean',\n",
    "         'totals.totalTransactionRevenue_month_sum_last',\n",
    "         'totals.totalTransactionRevenue_month_count_mean',\n",
    "         'totals.totalTransactionRevenue_month_count_last',\n",
    "         'totals.pageviews_month_sum_mean',\n",
    "         'totals.pageviews_month_sum_last',\n",
    "         'totals.pageviews_month_mean_mean',\n",
    "         'totals.pageviews_month_mean_last',\n",
    "         'totals.newVisits_month_sum_mean',\n",
    "         'totals.newVisits_month_sum_last',\n",
    "         'totals.newVisits_month_mean_mean',\n",
    "         'totals.newVisits_month_mean_last',\n",
    "         'totals.bounces_month_sum_mean',\n",
    "         'totals.bounces_month_sum_last']\n",
    "a_train=train_x[h]\n",
    "a_val=val_x[h]\n",
    "train_x=train_x.drop(h,axis=1)\n",
    "val_x=val_x.drop(h,axis=1)\n",
    "result=pd.DataFrame(columns=['Name','Score'])\n",
    "for col in h:\n",
    "    train_x[col]=a_train[col]\n",
    "    val_x[col]=a_val[col]\n",
    "    result.loc[len(result)]=[col,run_lgb(train_x,train_y,val_x,val_y,val_id)]\n",
    "    train_x=train_x.drop(col,axis=1)\n",
    "    val_x=val_x.drop(col,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
