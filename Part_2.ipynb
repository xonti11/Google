{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn import metrics\n",
    "from catboost import Pool,CatBoostRegressor,CatBoostClassifier\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "patch=\"D:\\Python\\\\Google\"\n",
    "def run_lgb(train_x, train_y, val_x, val_y,val_id):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"learning_rate\" : 0.02,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 50,\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(50/3), verbose_eval=100)\n",
    "    \n",
    "    pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_id.values})\n",
    "    val_pred_df[\"transactionRevenue\"] = val_y\n",
    "    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n",
    "    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].mean().reset_index()\n",
    "#    return model\n",
    "    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "def run_cat(train_x, train_y, val_x, val_y,val_id):\n",
    "    pool=Pool(train_x,train_y)\n",
    "    val_pool=Pool(val_x,val_y)\n",
    "    model = CatBoostRegressor(iterations=50, learning_rate=0.01,loss_function='RMSE', eval_metric='RMSE')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    \n",
    "    pred_val = model.predict(val_x)\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_id.values})\n",
    "    val_pred_df[\"transactionRevenue\"] = val_y\n",
    "    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n",
    "    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].mean().reset_index()\n",
    "#    return model\n",
    "    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "\n",
    "def run_cat_bin(train_x, train_y, val_x, val_y,val_id):\n",
    "    pool=Pool(train_x,train_y)\n",
    "    val_pool=Pool(val_x,val_y)\n",
    "    model = CatBoostClassifier(iterations=80, learning_rate=0.05,loss_function='CrossEntropy', eval_metric='AUC')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    pred_train=model.predict(train_x, prediction_type=\"Probability\")[:,1]\n",
    "    pred_val = model.predict(val_x, prediction_type=\"Probability\")[:,1]\n",
    "#    return val_pred_df\n",
    "    return  pred_train,pred_val\n",
    "\n",
    "def run_lgb_bin(train_x, train_y, val_x, val_y,val_id):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        \"learning_rate\" : 0.02,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 50,\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(50/3), verbose_eval=100)\n",
    "    \n",
    "    pred_train = model.predict(train_x, num_iteration=model.best_iteration)\n",
    "    pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    \n",
    "#    return model\n",
    "    return  pred_train,pred_val\n",
    "\n",
    "    \n",
    "def multimodel(train_x, train_y, val_x, val_y):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"learning_rate\" : 0.01,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 70,\n",
    "    }\n",
    "    i=random.randint(0,99999)\n",
    "    train1=pd.concat([train_x.reset_index(drop=True),pd.DataFrame(train_y,columns=['bg'])],axis=1)\n",
    "    train_x_bad=train1[train1['bg']>0].drop(['bg'],axis=1)\n",
    "    train_y_bad=train1[train1['bg']>0]['bg'].values\n",
    "    train_x_good=train1[train1['bg']==0].drop(['bg'],axis=1).sample(len(train_x_bad)*6,random_state=i)\n",
    "    train_y_good=train1[train1['bg']==0]['bg'].sample(len(train_x_bad)*6,random_state=i).values\n",
    "    \n",
    "    val1=pd.concat([val_x.reset_index(drop=True),pd.DataFrame(val_y,columns=['bg'])],axis=1)\n",
    "    val_x_bad=val1[val1['bg']>0].drop(['bg'],axis=1)\n",
    "    val_y_bad=val1[val1['bg']>0]['bg'].values\n",
    "    val_x_good=val1[val1['bg']==0].drop(['bg'],axis=1).sample(len(val_x_bad)*6,random_state=i)\n",
    "    val_y_good=val1[val1['bg']==0]['bg'].sample(len(val_x_bad)*6,random_state=i).values\n",
    "    \n",
    "    train_x,train_y=pd.concat([train_x_bad,train_x_good]),np.concatenate([train_y_bad,train_y_good])\n",
    "    val_x,val_y=pd.concat([val_x_bad,val_x_good]),np.concatenate([val_y_bad,val_y_good])\n",
    "#     pool=Pool(train_x,train_y)\n",
    "#     val_pool=Pool(val_x,val_y)\n",
    "#     model = CatBoostRegressor(iterations=50, learning_rate=0.01,loss_function='RMSE', eval_metric='RMSE')\n",
    "#     model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(50/3), verbose_eval=100)\n",
    "    return model\n",
    "\n",
    "def run_multimodel(train_x, train_y, val_x, val_y,val_id):\n",
    "    result_val=[]\n",
    "    for i in range(0,10):\n",
    "        model=multimodel(train_x, train_y, val_x, val_y)\n",
    "        result_val.append(model.predict(val_x, num_iteration=model.best_iteration))\n",
    "    pred_val = sum(result_val)/float(len(result_val))\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_id.values})\n",
    "    val_pred_df[\"transactionRevenue\"] = val_y\n",
    "    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n",
    "    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "#    return val_pred_df\n",
    "    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "def run_lgb2(train_x, train_y, val_x, val_y,val_id):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"learning_rate\" : 0.02,\n",
    "        'n_estimators': 50,\n",
    "         'num_iterations': 50,\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgval = lgb.Dataset(val_x, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgval], early_stopping_rounds=int(itr/3), verbose_eval=100)\n",
    "    \n",
    "    pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"transactionRevenue\":val_y,\"PredictedRevenue\":pred_val})\n",
    "#    return model\n",
    "    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "def run_cat2(train_x, train_y, val_x, val_y,val_id):\n",
    "    pool=Pool(train_x,train_y)\n",
    "    val_pool=Pool(val_x,val_y)\n",
    "    model = CatBoostRegressor(iterations=30, learning_rate=0.01,loss_function='RMSE', eval_metric='RMSE')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True,verbose_eval=False)\n",
    "    \n",
    "    pred_val = model.predict(val_x)\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"transactionRevenue\":val_y,\"PredictedRevenue\":pred_val})\n",
    "    return model\n",
    "#    return  np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_pickle(os.path.join(patch,'train_v2_clean.pkl'))\n",
    "test=pd.read_pickle(os.path.join(patch,'test_v2_clean.pkl'))\n",
    "train=train.append(test,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channelGrouping\n",
      "device.browser\n",
      "device.deviceCategory\n",
      "device.operatingSystem\n",
      "geoNetwork.city\n",
      "geoNetwork.continent\n",
      "geoNetwork.country\n",
      "geoNetwork.metro\n",
      "geoNetwork.networkDomain\n",
      "geoNetwork.region\n",
      "geoNetwork.subContinent\n",
      "trafficSource.adContent\n",
      "trafficSource.adwordsClickInfo.adNetworkType\n",
      "trafficSource.adwordsClickInfo.gclId\n",
      "trafficSource.adwordsClickInfo.page\n",
      "trafficSource.adwordsClickInfo.slot\n",
      "trafficSource.campaign\n",
      "trafficSource.keyword\n",
      "trafficSource.medium\n",
      "trafficSource.referralPath\n",
      "trafficSource.source\n",
      "trafficSource.adwordsClickInfo.isVideoAd\n",
      "trafficSource.isTrueDirect\n",
      "customDimensions_value\n",
      "weekday\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "#train=grouping_month(train)\n",
    "train[\"totals.totalTransactionRevenue\"].fillna(0, inplace=True)\n",
    "train[\"visitStartTime\"] = pd.to_datetime(train[\"visitStartTime\"],unit='s')\n",
    "\n",
    "train['day']=train['visitStartTime'].dt.day.astype('uint8')\n",
    "train['weekday']=train['visitStartTime'].apply(lambda x:x.date().weekday())\n",
    "train['minute']=train['visitStartTime'].dt.minute.astype('uint8')\n",
    "train['hour']=train['visitStartTime'].dt.hour.astype('uint8')\n",
    "\n",
    "# label encode the categorical variables and convert the numerical variables to float\n",
    "#ar=df['geoNetwork.networkDomain'].value_counts()[df['geoNetwork.networkDomain'].value_counts()<=4].index\n",
    "#train['geoNetwork.networkDomain']=['some unfamous domain' if x in ar else x for x in df['geoNetwork.networkDomain'].values]\n",
    "#del ar\n",
    "cat_cols = [\"channelGrouping\", \"device.browser\", \n",
    "            \"device.deviceCategory\", \"device.operatingSystem\", \n",
    "            \"geoNetwork.city\", \"geoNetwork.continent\", \n",
    "            \"geoNetwork.country\", \"geoNetwork.metro\",\n",
    "            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n",
    "            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n",
    "            \"trafficSource.adwordsClickInfo.adNetworkType\", \n",
    "            \"trafficSource.adwordsClickInfo.gclId\", \n",
    "            \"trafficSource.adwordsClickInfo.page\", \n",
    "            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n",
    "            \"trafficSource.keyword\", \"trafficSource.medium\", \n",
    "            \"trafficSource.referralPath\", \"trafficSource.source\",\n",
    "            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect',\n",
    "           'customDimensions_value','weekday']\n",
    "for col in cat_cols:\n",
    "    print(col)\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[col].values.astype('str')))\n",
    "    train[col] = lbl.transform(list(train[col].values.astype('str')))\n",
    "#train=pd.get_dummies(train,columns=cat_cols,prefix=cat_cols,prefix_sep='_')\n",
    "\n",
    "num_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", 'totals.bounces',  'totals.newVisits',\n",
    "            'totals.transactions','totals.sessionQualityDim','totals.timeOnSite','day','minute','hour']\n",
    "\n",
    "# num_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", 'totals.bounces',  'totals.newVisits',\n",
    "#            'totals.transactions','totals.sessionQualityDim','totals.timeOnSite','day','minute','hour',\n",
    "#             'totals.totalTransactionRevenue_month_sum',\n",
    "#            'totals.totalTransactionRevenue_month_count', 'totals.hits_month_sum',\n",
    "#            'totals.hits_month_count', 'totals.hits_month_median',\n",
    "#            'totals.pageviews_month_sum', 'totals.pageviews_month_mean',\n",
    "#            'totals.pageviews_month_median', 'totals.bounces_month_sum',\n",
    "#            'totals.bounces_month_mean', 'totals.bounces_month_median',\n",
    "#            'totals.newVisits_month_sum', 'totals.newVisits_month_mean',\n",
    "#            'totals.newVisits_month_median']    \n",
    "for col in num_cols:\n",
    "    train[col] = train[col].astype(float)\n",
    "    \n",
    "train['totals.totalTransactionRevenue'] = train['totals.totalTransactionRevenue'].astype(float)\n",
    "train['totals.totalTransactionCount'] = np.where(train['totals.totalTransactionRevenue'].values>0,1,0)\n",
    "train['device.isMobile']=train['device.isMobile'].astype(int)\n",
    "\n",
    "# Split the train dataset into development and valid based on time \n",
    "visitid=train['fullVisitorId']\n",
    "train=train.drop(['date','visitId','totals.transactionRevenue'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train[\"date\"]=pd.to_datetime(train[\"date\"], format='%Y%m%d')\n",
    "#train['Month']=train[\"date\"].dt.to_period(\"M\")\n",
    "def grouping_month(df):\n",
    "    df[\"date\"]=pd.to_datetime(df[\"visitStartTime\"], format='%Y%m%d')\n",
    "    df['Month']=df[\"date\"].dt.to_period(\"M\")\n",
    "    df['totals.hits']=df['totals.hits'].astype(int)\n",
    "    df['totals.newVisits'].fillna(0, inplace=True)\n",
    "    df['totals.newVisits']=df['totals.newVisits'].astype(int)\n",
    "    df['totals.pageviews'].fillna(0, inplace=True)\n",
    "    df['totals.pageviews']=df['totals.pageviews'].astype(int)\n",
    "    df['totals.bounces'].fillna(0, inplace=True)\n",
    "    df['totals.bounces']=df['totals.bounces'].astype(int)\n",
    "    df['totals.totalTransactionRevenue'].fillna(0, inplace=True)\n",
    "    df['totals.totalTransactionRevenue']=df['totals.totalTransactionRevenue'].astype(float)\n",
    "    aggs={\n",
    "        'totals.totalTransactionRevenue': ['sum','count'],\n",
    "        'totals.hits': ['sum','count', 'median'],\n",
    "        'totals.pageviews': ['sum','mean', 'median'],\n",
    "        'totals.bounces': ['sum', 'mean', 'median'],\n",
    "        'totals.newVisits': ['sum', 'mean', 'median']\n",
    "    }\n",
    "    temp=df.groupby(['fullVisitorId','Month'],as_index=False).agg({**aggs})\n",
    "    col=[\"_month_\".join(x) for x in temp.columns.ravel()[2:]]\n",
    "    col.insert(0,'Month')\n",
    "    col.insert(0,'fullVisitorId')\n",
    "    temp.columns=col\n",
    "    #temp.columns=['fullVisitorId','Month','transaction_month_count']\n",
    "    df=pd.merge(df,temp,on=['fullVisitorId','Month'],how='left')\n",
    "    return df\n",
    "def grouping_all(df):\n",
    "    aggs={\n",
    "        'totals.totalTransactionRevenue': ['sum'],\n",
    "        'totals.hits': ['sum','count', 'median'],\n",
    "        'totals.pageviews': ['sum','mean', 'median'],\n",
    "        'totals.bounces': ['sum', 'mean', 'median'],\n",
    "        'totals.newVisits': ['sum', 'mean', 'median']\n",
    "    }\n",
    "    temp=df.groupby(['fullVisitorId'],as_index=False).agg({**aggs})\n",
    "    col=[\"_all_\".join(x) for x in temp.columns.ravel()[1:]]\n",
    "    col.insert(0,'fullVisitorId')\n",
    "    temp.columns=col\n",
    "    #temp.columns=['fullVisitorId','Month','transaction_month_count']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "def grouping_date(df,y,m,d):\n",
    "    temp=df.groupby('fullVisitorId',as_index=False)['visitStartTime'].max()\n",
    "    temp.columns=['fullVisitorId','last_visit']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    temp=df[df['totals.totalTransactionRevenue']>0].groupby('fullVisitorId',as_index=False)['visitStartTime'].max()\n",
    "    temp.columns=['fullVisitorId','last_visit_money']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    dat_cols=['visitStartTime','last_visit','last_visit_money']\n",
    "    for col in dat_cols:\n",
    "        df[col] = pd.to_datetime(df[col],unit='s')\n",
    "    df['last_visit']=df['last_visit']-datetime.datetime(y,m,d)\n",
    "    df['last_visit']=df['last_visit'].dt.days\n",
    "    df['last_visit_money']=df['last_visit_money']-datetime.datetime(y,m,d)\n",
    "    df['last_visit_money']=df['last_visit_money'].dt.days\n",
    "    return df\n",
    "def grouping_date(df,y,m,d):\n",
    "    temp=df.groupby('fullVisitorId',as_index=False)['visitStartTime'].max()\n",
    "    temp.columns=['fullVisitorId','last_visit']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    temp=df[df['totals.totalTransactionRevenue']>0].groupby('fullVisitorId',as_index=False)['visitStartTime'].max()\n",
    "    temp.columns=['fullVisitorId','last_visit_money']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    \n",
    "    temp=df.groupby('fullVisitorId',as_index=False)['visitStartTime'].min()\n",
    "    temp.columns=['fullVisitorId','duration_visit']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    temp=df[df['totals.totalTransactionRevenue']>0].groupby('fullVisitorId',as_index=False)['visitStartTime'].min()\n",
    "    temp.columns=['fullVisitorId','duration_visit_money']\n",
    "    df=pd.merge(df,temp,on='fullVisitorId',how='left')\n",
    "    \n",
    "    dat_cols=['visitStartTime','last_visit','last_visit_money','duration_visit','duration_visit_money']\n",
    "    for col in dat_cols:\n",
    "        df[col] = pd.to_datetime(df[col],unit='s')\n",
    "    df['duration_visit']=df['last_visit']-df['duration_visit']\n",
    "    df['duration_visit']=df['duration_visit'].dt.days\n",
    "    df['duration_visit_money']=df['last_visit_money']-df['duration_visit_money']\n",
    "    df['duration_visit_money']=df['duration_visit_money'].dt.days\n",
    "    df['last_visit']=datetime.datetime(y,m,d)-df['last_visit']\n",
    "    df['last_visit']=df['last_visit'].dt.days\n",
    "    df['last_visit_money']=datetime.datetime(y,m,d)-df['last_visit_money']\n",
    "    df['last_visit_money']=df['last_visit_money'].dt.days\n",
    "    return df\n",
    "def grouping_id(df):\n",
    "    aggs={\n",
    "        'totals.totalTransactionRevenue': ['sum'],\n",
    "        'totals.totalTransactionCount': ['sum','mean'],\n",
    "        'totals.hits': ['count'],\n",
    "        'totals.pageviews': ['sum', 'median'],\n",
    "        'totals.bounces': ['sum'],\n",
    "        'totals.newVisits': ['sum', 'mean'],\n",
    "        'customDimensions_value': ['max','min'],\n",
    "        'geoNetwork.subContinent': ['max','min'],\n",
    "        'geoNetwork.region': ['max','min'],\n",
    "        'geoNetwork.networkDomain': ['max','min'],\n",
    "        'geoNetwork.continent': ['max','min'],\n",
    "        'device.operatingSystem': ['max','min'],\n",
    "        'device.deviceCategory': ['max','min'],\n",
    "        'channelGrouping': ['max','min'],\n",
    "        'visitNumber': ['max'],\n",
    "        'last_visit_money': ['max'],\n",
    "         'duration_visit': ['max']\n",
    "#        'totals.timeOnSite': ['mean'],\n",
    "#        'hour': ['max','mean','last'],\n",
    "#        'weekday': ['mean'],\n",
    "#         'duration_visit_money': ['max']\n",
    "    }\n",
    "    temp=df.groupby(['fullVisitorId'],as_index=False).agg({**aggs})\n",
    "    col=[\"_\".join(x) for x in temp.columns.ravel()[1:]]\n",
    "    col.insert(0,'fullVisitorId')\n",
    "    temp.columns=col\n",
    "    temp.fillna(0, inplace=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\lightgbm\\engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid_0's rmse: 0.890738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09722046751876291"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def super_data(train,a1,b1,c1,a2,b2,c2,a3,b3,c3,a4,b4,c4):\n",
    "    train_x_index=train[(train['visitStartTime']>=datetime.date(a1,b1,c1))&(train['visitStartTime']<datetime.date(a2,b2,c2))].index\n",
    "    train_y_index=train[(train['visitStartTime']>=datetime.date(a3,b3,c3))&(train['visitStartTime']<datetime.date(a4,b4,c4))].index\n",
    "    df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "    df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "    train_y=pd.merge(train.drop(['visitStartTime'],axis=1).loc[train_x_index],df_train_y,how='left',on='fullVisitorId')\n",
    "    train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "    train_x=train.drop(['visitStartTime','fullVisitorId'],axis=1).loc[train_x_index]\n",
    "    train_x=grouping_all(train.loc[train_x_index])   \n",
    "    train_x=train_x.drop(['visitStartTime','fullVisitorId'],axis=1)\n",
    "    return train_x,train_y\n",
    "import time\n",
    "import datetime\n",
    "train_x_index=train[(train['visitStartTime']>=datetime.date(2016,8,1))&(train['visitStartTime']<datetime.date(2017,2,1))].index\n",
    "train_y_index=train[(train['visitStartTime']>=datetime.date(2017,3,1))&(train['visitStartTime']<datetime.date(2017,5,1))].index\n",
    "val_x_index=train[(train['visitStartTime']>=datetime.date(2017,2,1))&(train['visitStartTime']<datetime.date(2017,8,1))].index\n",
    "val_y_index=train[(train['visitStartTime']>=datetime.date(2017,9,1))&(train['visitStartTime']<datetime.date(2018,11,1))].index\n",
    "\n",
    "df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "train_y=pd.merge(train.drop(['visitStartTime'],axis=1).loc[train_x_index],df_train_y,how='left',on='fullVisitorId')\n",
    "train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "train_x=train.drop(['visitStartTime','fullVisitorId'],axis=1).loc[train_x_index]\n",
    "train_x=grouping_all(train.loc[train_x_index])\n",
    "#train_x=grouping_date(train_x.loc[train_x_index],2016,11,1)\n",
    "train_x=train_x.drop(['visitStartTime','fullVisitorId'],axis=1)\n",
    "temp_x,temp_y=super_data(train,2017,3,1,2017,9,1,2017,10,1,2017,12,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "temp_x,temp_y=super_data(train,2017,10,1,2018,4,1,2018,5,1,2018,7,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "\n",
    "df_val_y=train.drop(['visitStartTime'],axis=1).loc[val_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_val_y=df_val_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "val_y=pd.merge(train.drop(['visitStartTime'],axis=1).loc[val_x_index],df_val_y,how='left',on='fullVisitorId')\n",
    "val_y=np.log1p(val_y['Revenue'].fillna(0).values)\n",
    "val_id=visitid[val_x_index]\n",
    "val_x=train.drop(['visitStartTime','fullVisitorId'],axis=1).loc[val_x_index]\n",
    "val_x=grouping_all(train.loc[val_x_index])\n",
    "#val_x=grouping_date(val_x.loc[val_x_index],2017,11,1)\n",
    "val_x=val_x.drop(['visitStartTime','fullVisitorId'],axis=1)\n",
    "\n",
    "run_lgb(train_x,train_y,val_x,val_y,val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\lightgbm\\engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05435599257575876"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def super_data(train,a1,b1,c1,a2,b2,c2,a3,b3,c3,a4,b4,c4):\n",
    "    train_x_index=train[(train['visitStartTime']>=datetime.date(a1,b1,c1))&(train['visitStartTime']<datetime.date(a2,b2,c2))].index\n",
    "    train_y_index=train[(train['visitStartTime']>=datetime.date(a3,b3,c3))&(train['visitStartTime']<datetime.date(a4,b4,c4))].index\n",
    "    df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "    df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "    df_train_x=grouping_id(grouping_date(train.loc[train_x_index],a2,b2,c2))\n",
    "    train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "    train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "    train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "    return train_x,train_y\n",
    "import time\n",
    "import datetime\n",
    "train_x_index=train[(train['visitStartTime']>=datetime.date(2016,8,1))&(train['visitStartTime']<datetime.date(2017,1,1))].index\n",
    "train_y_index=train[(train['visitStartTime']>=datetime.date(2017,2,1))&(train['visitStartTime']<datetime.date(2017,4,1))].index\n",
    "val_x_index=train[(train['visitStartTime']>=datetime.date(2018,2,1))&(train['visitStartTime']<datetime.date(2018,8,1))].index\n",
    "val_y_index=train[(train['visitStartTime']>=datetime.date(2018,9,1))&(train['visitStartTime']<datetime.date(2018,11,1))].index\n",
    "\n",
    "df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "df_train_x=grouping_id(grouping_date(train.loc[train_x_index],2017,2,1))\n",
    "train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "#train_x=grouping_date(train_x.loc[train_x_index],2016,11,1)\n",
    "temp_x,temp_y=super_data(train,2017,1,1,2017,6,1,2017,7,1,2017,9,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "temp_x,temp_y=super_data(train,2017,6,1,2017,11,1,2017,12,1,2018,2,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "\n",
    "df_val_y=train.drop(['visitStartTime'],axis=1).loc[val_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_val_y=df_val_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "df_val_x=grouping_id(grouping_date(train.loc[val_x_index],2018,8,1))\n",
    "val_y=pd.merge(df_val_x,df_val_y,how='left',on='fullVisitorId')\n",
    "val_y=np.log1p(val_y['Revenue'].fillna(0).values)\n",
    "val_id=visitid[val_x_index]\n",
    "val_x=df_val_x.drop(['fullVisitorId'],axis=1)\n",
    "\n",
    "run_lgb2(train_x,train_y,val_x,val_y,val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\lightgbm\\engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid_0's rmse: 0.323327\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[43]\tvalid_0's rmse: 0.324482\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[38]\tvalid_0's rmse: 0.324463\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\tvalid_0's rmse: 0.324592\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid_0's rmse: 0.324459\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[41]\tvalid_0's rmse: 0.324566\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[44]\tvalid_0's rmse: 0.324473\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[40]\tvalid_0's rmse: 0.32457\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[41]\tvalid_0's rmse: 0.3244\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[41]\tvalid_0's rmse: 0.324461\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[42]\tvalid_0's rmse: 0.324371\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[38]\tvalid_0's rmse: 0.324423\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.32451\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's rmse: 0.324754\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[41]\tvalid_0's rmse: 0.324365\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[48]\tvalid_0's rmse: 0.324294\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\tvalid_0's rmse: 0.325092\n",
      "Training until validation scores don't improve for 16 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tvalid_0's rmse: 0.324809\n"
     ]
    }
   ],
   "source": [
    "def super_data(train,a1,b1,c1,a2,b2,c2,a3,b3,c3,a4,b4,c4):\n",
    "    train_x_index=train[(train['visitStartTime']>=datetime.date(a1,b1,c1))&(train['visitStartTime']<datetime.date(a2,b2,c2))].index\n",
    "    train_y_index=train[(train['visitStartTime']>=datetime.date(a3,b3,c3))&(train['visitStartTime']<datetime.date(a4,b4,c4))].index\n",
    "    df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "    df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "    df_train_x=grouping_id(grouping_date(train.loc[train_x_index],a2,b2,c2))\n",
    "    train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "    train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "    train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "    return train_x,train_y\n",
    "import time\n",
    "import datetime\n",
    "train_x_index=train[(train['visitStartTime']>=datetime.date(2016,8,1))&(train['visitStartTime']<datetime.date(2017,1,1))].index\n",
    "train_y_index=train[(train['visitStartTime']>=datetime.date(2017,2,1))&(train['visitStartTime']<datetime.date(2017,4,1))].index\n",
    "val_x_index=train[(train['visitStartTime']>=datetime.date(2018,2,1))&(train['visitStartTime']<datetime.date(2018,8,1))].index\n",
    "val_y_index=train[(train['visitStartTime']>=datetime.date(2018,9,1))&(train['visitStartTime']<datetime.date(2018,11,1))].index\n",
    "\n",
    "df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "df_train_x=grouping_id(grouping_date(train.loc[train_x_index],2017,2,1))\n",
    "train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "#train_x=grouping_date(train_x.loc[train_x_index],2016,11,1)\n",
    "temp_x,temp_y=super_data(train,2017,1,1,2017,6,1,2017,7,1,2017,9,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "temp_x,temp_y=super_data(train,2017,6,1,2017,11,1,2017,12,1,2018,2,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "\n",
    "df_val_y=train.drop(['visitStartTime'],axis=1).loc[val_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_val_y=df_val_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "df_val_x=grouping_id(grouping_date(train.loc[val_x_index],2018,8,1))\n",
    "val_y=pd.merge(df_val_x,df_val_y,how='left',on='fullVisitorId')\n",
    "val_y=np.log1p(val_y['Revenue'].fillna(0).values)\n",
    "val_id=visitid[val_x_index]\n",
    "val_x=df_val_x.drop(['fullVisitorId'],axis=1)\n",
    "\n",
    "#run_lgb2(train_x,train_y,val_x,val_y,val_id)\n",
    "result=pd.DataFrame(columns=['Name','Score'])\n",
    "for col in train_x.columns:\n",
    "    result.loc[len(result)]=[col,run_lgb2(train_x.drop(col,axis=1),train_y,val_x.drop(col,axis=1),val_y,val_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_data(train,a1,b1,c1,a2,b2,c2,a3,b3,c3,a4,b4,c4):\n",
    "    train_x_index=train[(train['visitStartTime']>=datetime.date(a1,b1,c1))&(train['visitStartTime']<datetime.date(a2,b2,c2))].index\n",
    "    train_y_index=train[(train['visitStartTime']>=datetime.date(a3,b3,c3))&(train['visitStartTime']<datetime.date(a4,b4,c4))].index\n",
    "    df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "    df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "    df_train_x=grouping_id(grouping_date(train.loc[train_x_index],a2,b2,c2))\n",
    "    train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "    train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "    train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "    return train_x,train_y\n",
    "import time\n",
    "import datetime\n",
    "train_x_index=train[(train['visitStartTime']>=datetime.date(2016,8,1))&(train['visitStartTime']<datetime.date(2017,1,1))].index\n",
    "train_y_index=train[(train['visitStartTime']>=datetime.date(2017,2,1))&(train['visitStartTime']<datetime.date(2017,4,1))].index\n",
    "val_x_index=train[(train['visitStartTime']>=datetime.date(2018,2,1))&(train['visitStartTime']<datetime.date(2018,8,1))].index\n",
    "val_y_index=train[(train['visitStartTime']>=datetime.date(2018,9,1))&(train['visitStartTime']<datetime.date(2018,11,1))].index\n",
    "\n",
    "df_train_y=train.drop(['visitStartTime'],axis=1).loc[train_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_train_y=df_train_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "df_train_x=grouping_id(grouping_date(train.loc[train_x_index],2017,2,1))\n",
    "train_y=pd.merge(df_train_x,df_train_y,how='left',on='fullVisitorId')\n",
    "train_y=np.log1p(train_y['Revenue'].fillna(0).values)\n",
    "train_x=df_train_x.drop(['fullVisitorId'],axis=1)\n",
    "#train_x=grouping_date(train_x.loc[train_x_index],2016,11,1)\n",
    "temp_x,temp_y=super_data(train,2017,1,1,2017,6,1,2017,7,1,2017,9,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "temp_x,temp_y=super_data(train,2017,6,1,2017,11,1,2017,12,1,2018,2,1)\n",
    "train_x=train_x.append(temp_x)\n",
    "train_y=np.append(train_y,temp_y)\n",
    "\n",
    "df_val_y=train.drop(['visitStartTime'],axis=1).loc[val_y_index].groupby('fullVisitorId')['totals.totalTransactionRevenue'].sum().reset_index()\n",
    "df_val_y=df_val_y.rename(columns={'totals.totalTransactionRevenue':'Revenue'})\n",
    "df_val_x=grouping_id(grouping_date(train.loc[val_x_index],2018,8,1))\n",
    "val_y=pd.merge(df_val_x,df_val_y,how='left',on='fullVisitorId')\n",
    "val_y=np.log1p(val_y['Revenue'].fillna(0).values)\n",
    "val_id=visitid[val_x_index]\n",
    "val_x=df_val_x.drop(['fullVisitorId'],axis=1)\n",
    "result=pd.DataFrame(columns=['iteration','learning_rate','mean'])\n",
    "for itr in [5,10,20,30,40,50]:\n",
    "    for rate in [0.01]:\n",
    "    # for estim in [20,40,50,60,70,90,110,130,170]:\n",
    "    #     average=[]\n",
    "        result.loc[len(result)]=[itr,rate,run_cat2(train_x,train_y,val_x,val_y,val_id,itr,rate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.054319</td>\n",
       "      <td>totals.totalTransactionRevenue_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.610358</td>\n",
       "      <td>totals.totalTransactionCount_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.917608</td>\n",
       "      <td>totals.totalTransactionCount_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041738</td>\n",
       "      <td>totals.hits_count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.830850</td>\n",
       "      <td>totals.pageviews_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.002297</td>\n",
       "      <td>totals.pageviews_median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.830267</td>\n",
       "      <td>totals.bounces_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>totals.newVisits_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>totals.newVisits_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>customDimensions_value_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>customDimensions_value_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.subContinent_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.subContinent_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.region_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.265767</td>\n",
       "      <td>geoNetwork.region_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.735981</td>\n",
       "      <td>geoNetwork.networkDomain_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.networkDomain_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.continent_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>geoNetwork.continent_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.503641</td>\n",
       "      <td>device.operatingSystem_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.206225</td>\n",
       "      <td>device.operatingSystem_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>device.deviceCategory_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>device.deviceCategory_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.651300</td>\n",
       "      <td>channelGrouping_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>channelGrouping_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.620712</td>\n",
       "      <td>visitNumber_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.503026</td>\n",
       "      <td>last_visit_money_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17.008186</td>\n",
       "      <td>duration_visit_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>totals.timeOnSite_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.238583</td>\n",
       "      <td>totals.timeOnSite_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>totals.timeOnSite_sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>hour_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.940952</td>\n",
       "      <td>hour_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.605621</td>\n",
       "      <td>hour_last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.204099</td>\n",
       "      <td>weekday_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.228472</td>\n",
       "      <td>weekday_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>weekday_last</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                   1\n",
       "0   36.054319  totals.totalTransactionRevenue_sum\n",
       "1    3.610358    totals.totalTransactionCount_sum\n",
       "2    1.917608   totals.totalTransactionCount_mean\n",
       "3    0.041738                   totals.hits_count\n",
       "4    9.830850                totals.pageviews_sum\n",
       "5    3.002297             totals.pageviews_median\n",
       "6    0.830267                  totals.bounces_sum\n",
       "7    0.000000                totals.newVisits_sum\n",
       "8    0.000000               totals.newVisits_mean\n",
       "9    0.000000          customDimensions_value_max\n",
       "10   0.000000          customDimensions_value_min\n",
       "11   0.000000         geoNetwork.subContinent_max\n",
       "12   0.000000         geoNetwork.subContinent_min\n",
       "13   0.000000               geoNetwork.region_max\n",
       "14   1.265767               geoNetwork.region_min\n",
       "15   0.735981        geoNetwork.networkDomain_max\n",
       "16   0.000000        geoNetwork.networkDomain_min\n",
       "17   0.000000            geoNetwork.continent_max\n",
       "18   0.000000            geoNetwork.continent_min\n",
       "19   1.503641          device.operatingSystem_max\n",
       "20   0.206225          device.operatingSystem_min\n",
       "21   0.000000           device.deviceCategory_max\n",
       "22   0.000000           device.deviceCategory_min\n",
       "23   1.651300                 channelGrouping_max\n",
       "24   0.000000                 channelGrouping_min\n",
       "25   0.620712                     visitNumber_max\n",
       "26  13.503026                last_visit_money_max\n",
       "27  17.008186                  duration_visit_max\n",
       "28   0.000000               totals.timeOnSite_max\n",
       "29   1.238583              totals.timeOnSite_mean\n",
       "30   0.000000               totals.timeOnSite_sum\n",
       "31   0.000000                            hour_max\n",
       "32   1.940952                           hour_mean\n",
       "33   0.605621                           hour_last\n",
       "34   0.204099                         weekday_max\n",
       "35   4.228472                        weekday_mean\n",
       "36   0.000000                        weekday_last"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame(list(zip(model.feature_importances_,train_x.columns.values)))\n",
    "\n",
    "0.05435599257575876"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
